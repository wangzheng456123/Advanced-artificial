{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "对opscope提取tfidif特征完毕..........\n",
      "finished .............\n",
      "编码完毕................."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\miniconda3\\lib\\site-packages\\ipykernel_launcher.py:123: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\dell\\miniconda3\\lib\\site-packages\\ipykernel_launcher.py:126: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100%|████████████████████████████████████████████████████████████████████████| 24865/24865 [00:00<00:00, 138139.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|████████████████████████████████████████████████████████████████████████| 24865/24865 [00:00<00:00, 142086.14it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 24865/24865 [00:00<00:00, 138139.94it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 24865/24865 [00:00<00:00, 143730.22it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 24865/24865 [00:00<00:00, 148892.80it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 24865/24865 [00:00<00:00, 138912.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                                                positive_negtive\n",
      "09912c34159b1720558a419983a989f1dd2e0ed69a044ca3  中立                  6\n",
      "175ebe5f059ec050afbd65251ecdd3b512bfbe5e62d041b0  积极                  4\n",
      "                                                  中立                  3\n",
      "216bd2aaf4d079240c3ac0b76f0ef4aa355d443880ba78db  积极                  2\n",
      "                                                  中立                  1\n",
      "                                                                     ..\n",
      "f000950527a6feb6fe8f4850e9eb04e8ba3fa3b409725ef3  中立                  2\n",
      "f000950527a6feb6ff749dc50c7bf46b37b74e36ce38d1a4  消极                  1\n",
      "f000950527a6feb6ff7cdb55f5e64a477c499dd75137ae6b  积极                  2\n",
      "f000950527a6feb6ff839cdf509ebb7631857e6e363fedd6  中立                  1\n",
      "                                                  消极                  1\n",
      "Name: positive_negtive, Length: 1410, dtype: int64\n",
      "                                                   id positive non negtive\n",
      "0    09912c34159b1720558a419983a989f1dd2e0ed69a044ca3        0   6       0\n",
      "1    175ebe5f059ec050afbd65251ecdd3b512bfbe5e62d041b0        4   3       0\n",
      "2    216bd2aaf4d079240c3ac0b76f0ef4aa355d443880ba78db        2   1       0\n",
      "3    216bd2aaf4d079240f5823e63d24b44dd2c58e3281b822f6        0   2       0\n",
      "4    216bd2aaf4d0792410725ba5e7ca1dc32ce55767372f2030        0   0       1\n",
      "..                                                ...      ...  ..     ...\n",
      "921  f000950527a6feb6fd978de5c7f13afc9473089a63c1d3fc        2   1       0\n",
      "922  f000950527a6feb6fddefb42e3c3dc1932fc8c5fae14afbb        0   1       0\n",
      "923  f000950527a6feb6fe8f4850e9eb04e8ba3fa3b409725ef3        7   2       0\n",
      "924  f000950527a6feb6ff749dc50c7bf46b37b74e36ce38d1a4        0   0       1\n",
      "925  f000950527a6feb6ff7cdb55f5e64a477c499dd75137ae6b        2   0       0\n",
      "\n",
      "[926 rows x 4 columns]\n",
      "id\n",
      "f000950527a6feb6a83a6b740c21fe86cc936ff3902fb501    63\n",
      "f000950527a6feb69ae213e665a842c1e52002192fd39291    62\n",
      "516ab81418ed215d70d2992c79fb2c89f9bfd197e74ff49b    59\n",
      "f000950527a6feb6e856468300290e200429d5633a98e6e4    51\n",
      "f000950527a6feb68bfd4bea2542528d5c680ab0c989b1e9    51\n",
      "                                                    ..\n",
      "beb4aaaa89e0a0ae7949c231d735fd66f7c350d4a4e99bd7     1\n",
      "beb4aaaa89e0a0ae77f35cf02a28db4d71cb8232f1e7b327     1\n",
      "beb4aaaa89e0a0ae775be21385dc3e07cfdbc6169cc2bb4f     1\n",
      "beb4aaaa89e0a0ae74ceacde1d665f473b9ac610627733e2     1\n",
      "d8071a739aa75a3bfd192a4a4664b968e6406b8f66795e65     1\n",
      "Length: 8726, dtype: int64\n",
      "                                                    id number_changes\n",
      "0     f000950527a6feb6a83a6b740c21fe86cc936ff3902fb501             63\n",
      "1     f000950527a6feb69ae213e665a842c1e52002192fd39291             62\n",
      "2     516ab81418ed215d70d2992c79fb2c89f9bfd197e74ff49b             59\n",
      "3     f000950527a6feb6e856468300290e200429d5633a98e6e4             51\n",
      "4     f000950527a6feb68bfd4bea2542528d5c680ab0c989b1e9             51\n",
      "...                                                ...            ...\n",
      "8721  beb4aaaa89e0a0ae7949c231d735fd66f7c350d4a4e99bd7              1\n",
      "8722  beb4aaaa89e0a0ae77f35cf02a28db4d71cb8232f1e7b327              1\n",
      "8723  beb4aaaa89e0a0ae775be21385dc3e07cfdbc6169cc2bb4f              1\n",
      "8724  beb4aaaa89e0a0ae74ceacde1d665f473b9ac610627733e2              1\n",
      "8725  d8071a739aa75a3bfd192a4a4664b968e6406b8f66795e65              1\n",
      "\n",
      "[8726 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#coding: UTF-8 \n",
    "\n",
    "import os \n",
    "import re\n",
    "import pandas as pd\n",
    "import jieba\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import learning_curve, GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score,precision_recall_fscore_support,roc_curve,auc,roc_auc_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import feature_extraction\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "base_info=pd.read_csv('train/base_info.csv') #企业的基本信息\n",
    "annual_report_info=pd.read_csv('train/annual_report_info.csv') #企业的年报基本信息\n",
    "tax_info=pd.read_csv('train/tax_info.csv') #企业的纳税信息\n",
    "change_info=pd.read_csv('train/change_info.csv') #变更信息\n",
    "news_info=pd.read_csv('train/news_info.csv') #舆情信息\n",
    "other_info=pd.read_csv('train/other_info.csv') #其它信息\n",
    "entprise_info=pd.read_csv('train/entprise_info.csv') #企业标注信息{0: 13884, 1: 981}\n",
    "entprise_evaluate=pd.read_csv('entprise_evaluate.csv') #未标注信息\n",
    "\n",
    "# ## 1 特征构建 \n",
    "# ###  tfidi处理经营范围(opscope)特征\n",
    "\n",
    "# In[356]:\n",
    "\n",
    "\n",
    "# tfidif 处理经营范围的特征\n",
    "#cn_stopwords.txt来源于 https://github.com/goto456/stopwords\n",
    "def stopwordslist():\n",
    "    stopwords = [line.strip() for line in open('cn_stopwords.txt',encoding='UTF-8').readlines()]\n",
    "    return stopwords\n",
    "# 创建一个停用词列表\n",
    "stopwords = stopwordslist()\n",
    "stopwords+=['、', '；', '，', '）','（']\n",
    "#\n",
    "train_df_scope=base_info.merge(entprise_info)[['id','opscope','label']]\n",
    "test_df_scope=base_info[base_info['id'].isin(entprise_evaluate['id'].unique().tolist())]\n",
    "test_df_scope=test_df_scope.reset_index(drop=True)[['id','opscope']]\n",
    "str_label_0=''\n",
    "str_label_1=''\n",
    "for index,name,opscope,label in train_df_scope.itertuples():\n",
    "    # 结巴分词\n",
    "    seg_text = jieba.cut(opscope.replace(\"\\t\", \" \").replace(\"\\n\", \" \"))\n",
    "    outline = \" \".join(seg_text)\n",
    "    out_str=\"\"\n",
    "    for per in outline.split():\n",
    "        if per not in stopwords: \n",
    "            out_str += per\n",
    "            out_str+=\" \"\n",
    "    if label==0:\n",
    "        str_label_0+=out_str\n",
    "    else:\n",
    "        str_label_1+=out_str\n",
    "corpus=[str_label_0,str_label_1]\n",
    "vectorizer=CountVectorizer()#该类会将文本中的词语转换为词频矩阵，矩阵元素a[i][j] 表示j词在i类文本下的词频\n",
    "transformer=TfidfTransformer()#该类会统计每个词语的tf-idf权值\n",
    "tfidf=transformer.fit_transform(vectorizer.fit_transform(corpus))#第一个fit_transform是计算tf-idf，第二个fit_transform是将文本转为词频矩阵\n",
    "word=vectorizer.get_feature_names()#获取词袋模型中的所有词语总共7175个词语\n",
    "weight=tfidf.toarray()#将(2, 7175)tf-idf矩阵抽取出来，元素a[i][j]表示j词在i类文本中的tf-idf权重\n",
    "# for i in range(len(weight)):#打印每类文本的tf-idf词语权重，第一个for遍历所有文本，第二个for便利某一类文本下的词语权重\n",
    "#     #\n",
    "#     for j in range(len(word)):\n",
    "#         print(word[j],weight[i][j])\n",
    "#下面将会根据tfidi算出来的权重将经营范围的文本特征转换为数值(利用weight[1,:]也即各个词语在第二类(违法类中所占据的权重之和))\n",
    "illegal_word_weights={}\n",
    "for i in range(len(word)):\n",
    "    illegal_word_weights[word[i]]=weight[1][i]\n",
    "tfidi_opscope=[]\n",
    "for index,name,opscope in base_info[['id','opscope']].itertuples():\n",
    "    # \n",
    "    seg_text = jieba.cut(opscope.replace(\"\\t\", \" \").replace(\"\\n\", \" \"))\n",
    "    outline = \" \".join(seg_text)\n",
    "    tfidi_frt=0\n",
    "    for per in outline.split():\n",
    "        if per in illegal_word_weights: \n",
    "            tfidi_frt+=illegal_word_weights[per]\n",
    "    tfidi_opscope.append(tfidi_frt)\n",
    "base_info['tfidif_opscope']=tfidi_opscope\n",
    "print('对opscope提取tfidif特征完毕..........')\n",
    "\n",
    "\n",
    "# ##  change_info、other_info，news_info，annual_report_info,tax表格的简单特征构建\n",
    "\n",
    "# In[357]:\n",
    "\n",
    "\n",
    "#change_info\n",
    "change_info_clean_2 = change_info.drop(columns = ['bgrq', 'bgq', 'bgh'])\n",
    "change_info_clean_2 = change_info_clean_2.groupby('id',sort=False).agg('mean')\n",
    "change_info_clean_2 = pd.DataFrame(change_info_clean_2).reset_index()\n",
    "#other_info\n",
    "#空值大于0.5的列都删除掉\n",
    "buf_group = other_info.groupby('id',sort=False).agg('mean')\n",
    "other_info_clean=pd.DataFrame(buf_group).reset_index()\n",
    "other_info_clean=other_info_clean.fillna(-1)\n",
    "other_info_clean = other_info_clean.groupby('id',sort=False).agg('mean')\n",
    "other_info_clean=pd.DataFrame(other_info_clean).reset_index()\n",
    "#news_info\n",
    "news_info_clean_2=news_info.drop(['public_date'],axis=1)\n",
    "#对object类型进行编码\n",
    "news_info_clean_2['positive_negtive']=news_info_clean_2['positive_negtive'].fillna(\"中立\")\n",
    "#\n",
    "dic={}\n",
    "cate=news_info_clean_2.positive_negtive.unique()\n",
    "for i in range(len(cate)):\n",
    "    dic[cate[i]]=i\n",
    "\n",
    "news_info_clean_2['positive_negtive']=news_info_clean_2['positive_negtive'].map(dic)\n",
    "news_info_clean_2 = news_info_clean_2.groupby('id',sort=False).agg('mean')\n",
    "news_info_clean_2=pd.DataFrame(news_info_clean_2).reset_index()\n",
    "#处理annual_report_info的数据\n",
    "#空值大于0.5的列都删除掉\n",
    "annual_report_info_clean=annual_report_info.dropna(thresh=annual_report_info.shape[0]*0.5,how='all',axis=1)\n",
    "#对object类型进行编码\n",
    "annual_report_info_clean['BUSSTNAME']=annual_report_info_clean['BUSSTNAME'].fillna(\"无\")\n",
    "dic = {'无':-1,'开业':0, '歇业':1, '停业':2, '清算':3}\n",
    "#\n",
    "annual_report_info_clean['BUSSTNAME']=annual_report_info_clean['BUSSTNAME'].map(dic)\n",
    "annual_report_info_clean = annual_report_info_clean.groupby('id',sort=False).agg('mean')\n",
    "annual_report_info_clean=pd.DataFrame(annual_report_info_clean).reset_index()\n",
    "#处理tax数据\n",
    "tax_info_clean=tax_info.copy()\n",
    "tax_info_clean['START_DATE']=pd.to_datetime(tax_info_clean['START_DATE'])\n",
    "tax_info_clean['END_DATE']=pd.to_datetime(tax_info_clean['END_DATE'])\n",
    "tax_info_clean['gap_day']=(tax_info_clean['END_DATE']-tax_info_clean['START_DATE']).dt.total_seconds()//3600//24\n",
    "tax_info_clean=tax_info_clean.drop(['START_DATE','END_DATE'],axis=1)\n",
    "tax_info_clean['TAX_CATEGORIES']=tax_info_clean['TAX_CATEGORIES'].fillna(\"无\")#17 unique\n",
    "tax_info_clean['TAX_ITEMS']=tax_info_clean['TAX_ITEMS'].fillna(\"无\")#275 TAX_ITEMS\n",
    "#对object类型进行编码\n",
    "dic={}\n",
    "cate=tax_info_clean.TAX_CATEGORIES.unique()\n",
    "for i in range(len(cate)):\n",
    "    dic[cate[i]]=i\n",
    "tax_info_clean['TAX_CATEGORIES']=tax_info_clean['TAX_CATEGORIES'].map(dic)\n",
    "#\n",
    "dic={}\n",
    "cate=tax_info_clean.TAX_ITEMS.unique()\n",
    "for i in range(len(cate)):\n",
    "    dic[cate[i]]=i\n",
    "tax_info_clean['TAX_ITEMS']=tax_info_clean['TAX_ITEMS'].map(dic)\n",
    "tax_info_clean['income']=tax_info_clean['TAX_AMOUNT']/tax_info_clean['TAX_RATE']\n",
    "#\n",
    "tax_info_clean = tax_info_clean.groupby('id',sort=False).agg('mean')\n",
    "tax_info_clean=pd.DataFrame(tax_info_clean).reset_index()\n",
    "#税额分箱\n",
    "tax_info_clean['TAX_AMOUNT']=tax_info_clean['TAX_AMOUNT'].fillna(tax_info_clean['TAX_AMOUNT'].median())\n",
    "tax_info_clean['bucket_TAX_AMOUNT']=pd.qcut(tax_info_clean['TAX_AMOUNT'], 10, labels=False,duplicates='drop')\n",
    "print('finished .............')\n",
    "\n",
    "\n",
    "# ## base_info数据较为重要，需要构建诸多交叉特征以及特征分箱\n",
    "\n",
    "# In[358]:\n",
    "\n",
    "\n",
    "# #处理base_info数据\n",
    "base_info['opto']=pd.to_datetime(base_info['opto']).fillna(pd.to_datetime(base_info['opto']).max())\n",
    "base_info['opfrom']=pd.to_datetime(base_info['opfrom'])\n",
    "base_info['gap_year']=(base_info['opto']-base_info['opfrom']).dt.total_seconds()//3600//24//365\n",
    "base_info_clean=base_info.drop(['opscope','opfrom','opto'],axis=1)\n",
    "\n",
    "#............................对object类型进行编码...............................\n",
    "base_info_clean['industryphy']=base_info_clean['industryphy'].fillna(\"无\")\n",
    "base_info_clean['dom']=base_info_clean['dom'].fillna(\"无\")\n",
    "base_info_clean['opform']=base_info_clean['opform'].fillna(\"无\")\n",
    "base_info_clean['oploc']=base_info_clean['oploc'].fillna(\"无\")\n",
    "#\n",
    "dic={}\n",
    "cate=base_info_clean.industryphy.unique()\n",
    "for i in range(len(cate)):\n",
    "    dic[cate[i]]=i\n",
    "base_info_clean['industryphy']=base_info_clean['industryphy'].map(dic)\n",
    "#\n",
    "dic={}\n",
    "cate=base_info_clean.dom.unique()\n",
    "for i in range(len(cate)):\n",
    "    dic[cate[i]]=i\n",
    "base_info_clean['dom']=base_info_clean['dom'].map(dic)\n",
    "#\n",
    "dic={}\n",
    "cate=base_info_clean.opform.unique()\n",
    "for i in range(len(cate)):\n",
    "    dic[cate[i]]=i\n",
    "base_info_clean['opform']=base_info_clean['opform'].map(dic)\n",
    "#\n",
    "dic={}\n",
    "cate=base_info_clean.oploc.unique()\n",
    "for i in range(len(cate)):\n",
    "    dic[cate[i]]=i\n",
    "base_info_clean['oploc']=base_info_clean['oploc'].map(dic)\n",
    "#\n",
    "base_info_clean=base_info_clean.fillna(-1)\n",
    "#\n",
    "print('编码完毕.................')\n",
    "#........................分箱.................................\n",
    "def bucket(name,bucket_len):\n",
    "    gap_list=[base_info_clean[name].quantile(i/bucket_len) for i in range(bucket_len+1)]#以分位数作为分箱标志\n",
    "    len_data=len(base_info_clean[name])\n",
    "    new_col=[]\n",
    "    for i in base_info_clean[name].values:\n",
    "        for j in range(len(gap_list)):\n",
    "            if gap_list[j]>=i:\n",
    "                encode=j\n",
    "                break\n",
    "        new_col.append(encode)\n",
    "    return new_col\n",
    "#注册资本_实缴资本\n",
    "base_info_clean['regcap_reccap']=base_info_clean['regcap']-base_info_clean['reccap']\n",
    "#注册资本分箱\n",
    "base_info_clean['regcap']=base_info_clean['regcap'].fillna(base_info_clean['regcap'].median())\n",
    "base_info_clean['bucket_regcap']=pd.qcut(base_info_clean['regcap'], 10, labels=False,duplicates='drop')\n",
    "#实缴资本分箱\n",
    "base_info_clean['reccap']=base_info_clean['reccap'].fillna(base_info_clean['reccap'].median())\n",
    "base_info_clean['bucket_reccap']=pd.qcut(base_info_clean['reccap'], 10, labels=False,duplicates='drop')\n",
    "#注册资本_实缴资本分箱\n",
    "base_info_clean['regcap_reccap']=base_info_clean['regcap_reccap'].fillna(base_info_clean['regcap_reccap'].median())\n",
    "base_info_clean['bucket_regcap_reccap']=pd.qcut(base_info_clean['regcap_reccap'], 10, labels=False,duplicates='drop')\n",
    "#.............................交叉.........................\n",
    "#作两个特征的交叉\n",
    "def cross_two(name_1,name_2):\n",
    "    new_col=[]\n",
    "    encode=0\n",
    "    dic={}\n",
    "    val_1=base_info_clean[name_1]\n",
    "    val_2=base_info_clean[name_2]\n",
    "    for i in tqdm(range(len(val_1))):\n",
    "        tmp=str(val_1[i])+'_'+str(val_2[i])\n",
    "        if tmp in dic:\n",
    "            new_col.append(dic[tmp])\n",
    "        else:\n",
    "            dic[tmp]=encode\n",
    "            new_col.append(encode)\n",
    "            encode+=1\n",
    "    return new_col\n",
    "#作企业类型-小类的交叉特征\n",
    "base_info_clean['enttypegb']=base_info_clean['enttypegb'].fillna(\"无\")\n",
    "base_info_clean['enttypeitem']=base_info_clean['enttypeitem'].fillna(\"无\")\n",
    "new_col=cross_two('enttypegb','enttypeitem')#作企业类型-小类的交叉特征\n",
    "base_info_clean['enttypegb_enttypeitem']=new_col\n",
    "#\n",
    "#行业类别-细类的交叉特征\n",
    "base_info_clean['industryphy']=base_info_clean['industryphy'].fillna(\"无\")\n",
    "base_info_clean['industryco']=base_info_clean['industryco'].fillna(\"无\")\n",
    "new_col=cross_two('industryphy','industryco')#作企业类型-小类的交叉特征\n",
    "base_info_clean['industryphy_industryco']=new_col\n",
    "#企业类型-行业类别的交叉特征\n",
    "new_col=cross_two('enttypegb','industryphy')#作企业类型-小类的交叉特征\n",
    "base_info_clean['enttypegb_industryphy']=new_col\n",
    "#行业类别-企业类型小类的交叉特征\n",
    "new_col=cross_two('industryphy','enttypeitem')#作企业类型-小类的交叉特征\n",
    "base_info_clean['industryphy_enttypeitem']=new_col\n",
    "#行业类别细类--企业类型小类的交叉特征\n",
    "new_col=cross_two('industryco','enttypeitem')#作企业类型-小类的交叉特征\n",
    "base_info_clean['industryco_enttypeitem']=new_col\n",
    "\n",
    "#企业类型-小类-行业类别-细类的交叉特征\n",
    "new_col=cross_two('enttypegb_enttypeitem','industryphy_industryco')#作企业类型-小类的交叉特征\n",
    "base_info_clean['enttypegb_enttypeitem_industryphy_industryco']=new_col\n",
    "base_info_clean.shape\n",
    "\n",
    "\n",
    "# ## category特征单独提取出来\n",
    "\n",
    "# In[359]:\n",
    "\n",
    "\n",
    "cat_features=['industryphy','dom','opform','oploc','bucket_regcap',\n",
    "              'bucket_reccap','bucket_regcap_reccap',\n",
    "              'enttypegb','enttypeitem','enttypegb_enttypeitem',\n",
    "              'enttypegb_industryphy','enttypegb_enttypeitem_industryphy_industryco',\n",
    "              'industryphy','industryco','industryphy_industryco',\n",
    "              'industryphy_enttypeitem','industryco_enttypeitem',\n",
    "              'adbusign','townsign','regtype','TAX_CATEGORIES','bucket_TAX_AMOUNT',\n",
    "              'legal_judgment_num','brand_num','patent_num'\n",
    "             ]\n",
    "\n",
    "#处理舆论信息\n",
    "groups = news_info.groupby('id')\n",
    "\n",
    "news_info_clean = pd.DataFrame(columns = ['id', 'positive', 'non', 'negtive'])\n",
    "\n",
    "values = groups['positive_negtive'].value_counts()\n",
    "print(values)\n",
    "\n",
    "i = 0\n",
    "l = 0\n",
    "pre = ''\n",
    "tem = ['', 0, 0, 0]\n",
    "#print(values.index)\n",
    "for name in values.index:\n",
    "    if name[0] == pre:\n",
    "        if name[1] == '积极':\n",
    "            tem[1] = values[i]\n",
    "        if name[1] == '中立':\n",
    "            tem[2] = values[i]\n",
    "        if name[1] == '消极':\n",
    "            tem[3] = values[i]\n",
    "        i = i + 1\n",
    "        continue   \n",
    "    if i != 0:\n",
    "        tem[0] = pre\n",
    "        news_info_clean.loc[l] = tem\n",
    "        l = l + 1\n",
    "    pre = name[0]\n",
    "    tem = ['', 0, 0, 0]\n",
    "    if name[1] == '积极':\n",
    "        tem[1] = values[i]\n",
    "    if name[1] == '中立':\n",
    "        tem[2] = values[i]\n",
    "    if name[1] == '消极':\n",
    "        tem[3] = values[i]\n",
    "    i = i + 1\n",
    "print(news_info_clean)    \n",
    "\n",
    "#处理变更信息\n",
    "values = change_info.value_counts('id')\n",
    "print(values)\n",
    "\n",
    "change_info_clean = pd.DataFrame(columns = ['id', 'number_changes'])\n",
    "tem = ['', 0]\n",
    "i = 0\n",
    "for name in values.index:\n",
    "    tem[0] = name\n",
    "    tem[1] = values[i]\n",
    "    change_info_clean.loc[i] = tem\n",
    "    tem = ['', 0]\n",
    "    i = i + 1\n",
    "print(change_info_clean)    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators: 60\n",
      "[60, 8, 12] 0.8290513110411092\n",
      "[60, 8, 11] 0.830366626918495\n",
      "[60, 8, 15] 0.8383219649860969\n",
      "n_estimators: 50\n",
      "[50, 8, 15] 0.8383901017644865\n",
      "n_estimators: 55\n",
      "[55, 8, 15] 0.8397975051424035\n",
      "n_estimators: 65\n",
      "每0次验证的f1:0.816\n",
      "每1次验证的f1:0.8358974358974359\n",
      "每2次验证的f1:0.8258706467661691\n",
      "每3次验证的f1:0.8253164556962025\n",
      "每4次验证的f1:0.8396946564885496\n",
      "mean f1: 0.8285558389696714\n"
     ]
    }
   ],
   "source": [
    "all_data=base_info_clean.merge(annual_report_info_clean,how='outer')\n",
    "all_data=all_data.merge(tax_info_clean,how='outer')\n",
    "all_data=all_data.merge(change_info_clean,how='outer')\n",
    "all_data=all_data.merge(news_info_clean,how='outer')\n",
    "#all_data=all_data.merge(change_info_clean_2,how='outer')\n",
    "#all_data=all_data.merge(news_info_clean_2,how='outer')\n",
    "all_data=all_data.merge(other_info_clean,how='outer')\n",
    "all_data=all_data.fillna(-1)\n",
    "all_data[cat_features]=all_data[cat_features].astype(int)\n",
    "all_data.shape#,base_info.shape,annual_report_info.shape,tax_info.shape\n",
    "\n",
    "\n",
    "# In[361]:\n",
    "\n",
    "\n",
    "#\n",
    "train_df=all_data.merge(entprise_info)\n",
    "train_data=train_df.drop(['id','label'],axis=1)\n",
    "kind=train_df['label']\n",
    "test_df=all_data[all_data['id'].isin(entprise_evaluate['id'].unique().tolist())]\n",
    "test_df=test_df.reset_index(drop=True)\n",
    "test_data=test_df.drop(['id'],axis=1)\n",
    "train_data.shape,test_data.shape\n",
    "\n",
    "\n",
    "#特征筛选\n",
    "#frt_select=[\n",
    "#  'industryphy',\n",
    "#  'enttypegb',\n",
    "#  'regcap',\n",
    "#  'townsign',\n",
    "#  'industryco',\n",
    "#  'bucket_regcap',\n",
    "#  'empnum',\n",
    "#  'bucket_reccap',\n",
    "#  'enttypeitem',\n",
    "#  'industryphy_industryco',\n",
    "#  'reccap',\n",
    "#  'FORINVESTSIGN',\n",
    "#  'positive_negtive',\n",
    "#  'regtype',\n",
    "#  'STOCKTRANSIGN',\n",
    "#  'bucket_regcap_reccap',\n",
    "#  'enttypegb_enttypeitem',\n",
    "#  'regcap_reccap',\n",
    "#  'legal_judgment_num',\n",
    "#  'TAX_CATEGORIES',\n",
    "#  'TAX_AMOUNT',\n",
    "#  'bgq_bgh',\n",
    "#  'TAX_ITEMS',\n",
    "#  'positive',\n",
    "#  'negtive',\n",
    "#  'number_changes' ]\n",
    "# frt_select=important_frt[:30]\n",
    "#train_data=train_data[frt_select]\n",
    "#test_data=test_data[frt_select]\n",
    "#cat_features=list(set(frt_select).intersection(set(cat_features)))\n",
    "# cat_features\n",
    "\n",
    "\n",
    "def eval_score(y_test,y_pre):\n",
    "    _,_,f_class,_=precision_recall_fscore_support(y_true=y_test,y_pred=y_pre,labels=[0,1],average=None)\n",
    "    fper_class={'合法':f_class[0],'违法':f_class[1],'f1':f1_score(y_test,y_pre)}\n",
    "    return fper_class\n",
    "#\n",
    "def k_fold_serachParmaters(model,train_val_data,train_val_kind):\n",
    "    mean_f1=0\n",
    "    mean_f1Train=0\n",
    "    n_splits=5\n",
    "    sk = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=2020)\n",
    "    for train, test in sk.split(train_val_data, train_val_kind):\n",
    "        x_train = train_val_data.iloc[train]\n",
    "        y_train = train_val_kind.iloc[train]\n",
    "        x_test = train_val_data.iloc[test]\n",
    "        y_test = train_val_kind.iloc[test]\n",
    "\n",
    "        model.fit(x_train, y_train)\n",
    "        pred = model.predict(x_test)\n",
    "        fper_class =  eval_score(y_test,pred)\n",
    "        mean_f1+=fper_class['f1']/n_splits\n",
    "        #print(fper_class)\n",
    "        \n",
    "        pred_Train = model.predict(x_train)\n",
    "        fper_class_train =  eval_score(y_train,pred_Train)\n",
    "        mean_f1Train+=fper_class_train['f1']/n_splits\n",
    "    #print('mean valf1:',mean_f1)\n",
    "    #print('mean trainf1:',mean_f1Train)\n",
    "    return mean_f1\n",
    "\n",
    "\n",
    "def search_param(n_estimators,max_depth,min_samples_split):\n",
    "    rf = RandomForestClassifier(oob_score=True, random_state=2020,\n",
    "                    n_estimators= n_estimators,max_depth=max_depth,min_samples_split=min_samples_split)\n",
    "    mean_f1=k_fold_serachParmaters(rf,train_data,kind)\n",
    "    return mean_f1\n",
    "\n",
    "# #搜索最佳参数\n",
    "param=[]\n",
    "best=0\n",
    "for n_estimators in [60,50,55,65]:\n",
    "    print('n_estimators:',n_estimators)\n",
    "    for min_samples_split in [8,10,20,15]:\n",
    "        for max_depth in [12,11,13,15]:\n",
    "            mean_f1=search_param(n_estimators,max_depth,min_samples_split)\n",
    "            if mean_f1>best:\n",
    "                param=[n_estimators,min_samples_split,max_depth]\n",
    "                best=mean_f1\n",
    "                print(param,best)\n",
    "\n",
    "rf = RandomForestClassifier(oob_score=True, random_state=2020,\n",
    "            n_estimators= 60,max_depth=13,min_samples_split=10)\n",
    "k_fold_serachParmaters(rf,train_data,kind)\n",
    "\n",
    "model=rf#仅用随机森林\n",
    "details = []\n",
    "answers = []\n",
    "mean_f1=0\n",
    "n_splits=5\n",
    "sk = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=2020)\n",
    "cnt=0\n",
    "for train, test in sk.split(train_data, kind):\n",
    "    x_train = train_data.iloc[train]\n",
    "    y_train = kind.iloc[train]\n",
    "    x_test = train_data.iloc[test]\n",
    "    y_test = kind.iloc[test]\n",
    "\n",
    "    model.fit(x_train, y_train)\n",
    "    pred_cab = model.predict(x_test)\n",
    "    weight_cab =  eval_score(y_test,pred_cab)['f1']\n",
    "\n",
    "    print('每{}次验证的f1:{}'.format(cnt,weight_cab))\n",
    "    cnt+=1\n",
    "    mean_f1+=weight_cab/n_splits\n",
    "    ans = model.predict_proba(test_data)\n",
    "\n",
    "    answers.append(ans)\n",
    "print('mean f1:',mean_f1)\n",
    "\n",
    "fina=np.sqrt(sum(np.array(answers)**2)/n_splits)#平方平均\n",
    "fina=fina[:,1]\n",
    "test_df['score']=fina#可选:fina_persudo是伪标签的预测结果\n",
    "submit_csv=test_df[['id','score']]\n",
    "submit_csv.to_csv('submit.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
